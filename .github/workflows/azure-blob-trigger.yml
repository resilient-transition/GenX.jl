name: Azure Blob Storage GenX Pipeline

# This workflow can be manually triggered or via webhook
# For blob storage triggers, you'll need to set up an Azure Event Grid subscription
on:
  repository_dispatch:
    types: [blob-upload]
  workflow_dispatch:
    inputs:
      blob_container:
        description: 'Azure Blob Storage container name'
        required: true
        default: 'genx-data'
      input_folder:
        description: 'Input folder path in blob storage'
        required: true
        default: 'inputs'
      case_name:
        description: 'Case name for the GenX run'
        required: true
        default: 'test_case'

env:
  AZURE_STORAGE_ACCOUNT: ${{ secrets.AZURE_STORAGE_ACCOUNT }}
  AZURE_STORAGE_KEY: ${{ secrets.AZURE_STORAGE_KEY }}
  BLOB_CONTAINER: ${{ github.event.inputs.blob_container || github.event.client_payload.container || 'genx-data' }}
  INPUT_FOLDER: ${{ github.event.inputs.input_folder || github.event.client_payload.input_folder || 'inputs' }}
  CASE_NAME: ${{ github.event.inputs.case_name || github.event.client_payload.case_name || 'test_case' }}

jobs:
  run-genx:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Set up Julia
      uses: julia-actions/setup-julia@v1
      with:
        version: '1.11'  # Using recommended version from README

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install Azure CLI
      run: |
        curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install azure-storage-blob pandas loguru upath

    - name: Cache Julia dependencies
      uses: julia-actions/cache@v1

    - name: Install Julia dependencies
      run: |
        julia --project=. -e 'using Pkg; Pkg.instantiate()'

    - name: Download input data from Azure Blob Storage
      run: |
        # Login to Azure using storage account key
        az storage blob download-batch \
          --destination ./cases/${{ env.CASE_NAME }} \
          --source ${{ env.BLOB_CONTAINER }} \
          --pattern "${{ env.INPUT_FOLDER }}/*" \
          --account-name ${{ env.AZURE_STORAGE_ACCOUNT }} \
          --account-key ${{ env.AZURE_STORAGE_KEY }}
        
        echo "Downloaded input files:"
        find ./cases/${{ env.CASE_NAME }} -type f -name "*.csv" | head -10

    - name: Validate input files
      run: |
        # Check if required input files exist
        if [ ! -d "./cases/${{ env.CASE_NAME }}" ]; then
          echo "Error: Case directory not found"
          exit 1
        fi
        
        # List downloaded files for debugging
        echo "Case directory contents:"
        ls -la ./cases/${{ env.CASE_NAME }}/

    - name: Run GenX model
      run: |
        echo "Starting GenX model run for case: ${{ env.CASE_NAME }}"
        echo "Working directory: $(pwd)"
        
        # Run the GenX model
        julia --project=. Run.jl "./cases/${{ env.CASE_NAME }}"
        
        echo "GenX model run completed"

    - name: Prepare results for upload
      run: |
        # Create results directory structure
        mkdir -p ./upload_results
        
        # Copy all results files
        if [ -d "./cases/${{ env.CASE_NAME }}/results" ]; then
          cp -r ./cases/${{ env.CASE_NAME }}/results/* ./upload_results/
        else
          echo "Warning: No results directory found"
          # Look for results in alternative locations
          find ./cases/${{ env.CASE_NAME }} -name "*.csv" -path "*/results/*" -exec cp {} ./upload_results/ \;
        fi
        
        # Create a run summary
        echo "GenX Run Summary" > ./upload_results/run_summary.txt
        echo "Case Name: ${{ env.CASE_NAME }}" >> ./upload_results/run_summary.txt
        echo "Run Date: $(date)" >> ./upload_results/run_summary.txt
        echo "Commit SHA: ${{ github.sha }}" >> ./upload_results/run_summary.txt
        echo "" >> ./upload_results/run_summary.txt
        echo "Results Files:" >> ./upload_results/run_summary.txt
        ls -la ./upload_results/ >> ./upload_results/run_summary.txt
        
        echo "Prepared results for upload:"
        ls -la ./upload_results/

    - name: Upload results to Azure Blob Storage
      run: |
        # Create results folder with timestamp
        TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
        RESULTS_FOLDER="results/${{ env.CASE_NAME }}_${TIMESTAMP}"
        
        # Upload all results files
        az storage blob upload-batch \
          --destination ${{ env.BLOB_CONTAINER }} \
          --destination-path "$RESULTS_FOLDER" \
          --source ./upload_results \
          --account-name ${{ env.AZURE_STORAGE_ACCOUNT }} \
          --account-key ${{ env.AZURE_STORAGE_KEY }}
        
        echo "Results uploaded to: $RESULTS_FOLDER"
        
        # List uploaded files for confirmation
        az storage blob list \
          --container-name ${{ env.BLOB_CONTAINER }} \
          --prefix "$RESULTS_FOLDER" \
          --account-name ${{ env.AZURE_STORAGE_ACCOUNT }} \
          --account-key ${{ env.AZURE_STORAGE_KEY }} \
          --output table

    - name: Create job summary
      run: |
        echo "## GenX Model Run Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Case Name:** ${{ env.CASE_NAME }}" >> $GITHUB_STEP_SUMMARY
        echo "**Input Source:** ${{ env.BLOB_CONTAINER }}/${{ env.INPUT_FOLDER }}" >> $GITHUB_STEP_SUMMARY
        echo "**Results Location:** ${{ env.BLOB_CONTAINER }}/results/${{ env.CASE_NAME }}_$(date +"%Y%m%d_%H%M%S")" >> $GITHUB_STEP_SUMMARY
        echo "**Run Status:** âœ… Completed Successfully" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Results Files" >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
        ls -la ./upload_results/ >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY

    - name: Cleanup
      if: always()
      run: |
        # Clean up temporary files
        rm -rf ./cases/${{ env.CASE_NAME }}
        rm -rf ./upload_results

